{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About Dataset\n",
    "\n",
    "Global Crypto Currency Database\n",
    "\n",
    "The Global Crypto Currency Database is a comprehensive and meticulously curated dataset that offers a wealth of information on over 7500 cryptocurrencies, each paired with the US Dollar (USD). This dataset is an invaluable resource for anyone interested in exploring the world of digital currencies and analyzing their market behavior. These not only include popular coins such as BTC, ETH, and SOL but it also captures newly released coins as well.\n",
    "Dataset File Structure\n",
    "\n",
    "The Dataset is structured with the following key fields:\n",
    "\n",
    "Name: Coin Name\n",
    "\n",
    "    Type: String\n",
    "    Description: The official name of the cryptocurrency, enabling easy identification and reference to specific digital coins.\n",
    "\n",
    "Symbol: Trading Symbol of the Coin\n",
    "\n",
    "    Type: String\n",
    "    Description: This field provides the unique trading symbol associated with each cryptocurrency, a vital element for traders and investors.\n",
    "\n",
    "Date: Date of the Price\n",
    "\n",
    "    Type: Datetime\n",
    "    Description: Accurate time-stamping allows for precise tracking of cryptocurrency prices, facilitating trend analysis and historical comparisons.\n",
    "\n",
    "Open: Opening Price of the Day\n",
    "\n",
    "    Type: Number\n",
    "    Description: The opening price signifies the value at which the cryptocurrency began trading on a particular day, offering insights into market sentiment.\n",
    "\n",
    "High: Highest Price of the Day\n",
    "\n",
    "    Type: Number\n",
    "    Description: The highest price recorded during the day provides a glimpse into the cryptocurrency's peak performance within a given timeframe.\n",
    "\n",
    "Low: Lowest Price of the Day\n",
    "\n",
    "    Type: Number\n",
    "    Description: The lowest price registered during the day offers a perspective on the cryptocurrency's lowest trading point within that period.\n",
    "\n",
    "Close: Closing Price of the Day\n",
    "\n",
    "    Type: Number\n",
    "    Description: The closing price represents the cryptocurrency's final trading value for the day, crucial for assessing daily market performance.\n",
    "\n",
    "Adj Close: Adjusted Closing Price of the Day\n",
    "\n",
    "    Type: Number\n",
    "    Description: This field accounts for various factors such as dividends and stock splits, offering a more accurate view of a cryptocurrency's closing price.\n",
    "\n",
    "Potential Uses\n",
    "1. Investment Analysis\n",
    "\n",
    "Investors can leverage this dataset to analyze historical price trends, volatility, and correlations between cryptocurrencies and traditional assets. It helps in making informed investment decisions and managing risk in cryptocurrency portfolios.\n",
    "2. Market Research\n",
    "\n",
    "Market researchers can use this dataset to study market dynamics, identify emerging cryptocurrencies, and assess the impact of news events on cryptocurrency prices. It aids in understanding market sentiment and behavior.\n",
    "3. Algorithmic Trading\n",
    "\n",
    "Traders and quantitative analysts can develop algorithmic trading strategies based on historical price data. The dataset enables the backtesting of trading algorithms to assess their effectiveness.\n",
    "4. Risk Management\n",
    "\n",
    "Risk managers can assess the risk associated with cryptocurrency investments by analyzing historical price volatility and correlations with other asset classes.\n",
    "5. Academic Research\n",
    "\n",
    "Academics and researchers can use this dataset to conduct studies on various aspects of the cryptocurrency market, contributing to the academic understanding of digital currencies.\n",
    "Conclusion\n",
    "\n",
    "The Global Crypto Currency Database is a versatile and informative dataset that opens doors to a wide range of applications in the world of cryptocurrencies. Its structured and detailed information allows users to gain insights, make data-driven decisions, and explore the ever-evolving landscape of digital assets. Whether you're an investor, researcher, trader, or enthusiast, this dataset is a valuable tool for navigating the complexities of the cryptocurrency market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('/kaggle/input/global-cryptocurrency-price-database/metadata.csv')\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example currency\n",
    "currancy_name = 'Bitcoin USD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = meta.loc[meta['Coin Pair Name'] == currancy_name].iloc[:, 2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join('/kaggle/input/global-cryptocurrency-price-database/data', file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA (GPU) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the \"Close\" prices (the target variable)\n",
    "close_prices = data['Close'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "close_prices_scaled = scaler.fit_transform(close_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create input sequences and their corresponding target values\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    target = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i + seq_length]\n",
    "        label = data[i + seq_length]\n",
    "        sequences.append(seq)\n",
    "        target.append(label)\n",
    "    return np.array(sequences), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the sequence length and split the data into sequences and targets\n",
    "sequence_length = 10  # You can adjust this value\n",
    "sequences, target = create_sequences(close_prices_scaled, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "target = torch.tensor(target, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(sequences) * split_ratio)\n",
    "X_train, y_train = sequences[:split_index], target[:split_index]\n",
    "X_test, y_test = sequences[split_index:], target[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "input_size = 1\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training and testing\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = GRUModel(input_size, hidden_size, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define empty lists to store training history\n",
    "train_loss_history = []\n",
    "validation_loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Calculate training loss\n",
    "    train_loss = train(model, train_loader, criterion, optimizer)\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    # Calculate validation loss\n",
    "    #validation_loss = validate(model, validation_loader, criterion)\n",
    "    #validation_loss_history.append(validation_loss)\n",
    "\n",
    "# Update the training plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "#plt.plot(validation_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Progress')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_loss += criterion(outputs, labels).item()\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test.to(device))\n",
    "    predictions = scaler.inverse_transform(predictions.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original and predicted prices\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test, label='True Prices', color='blue')\n",
    "plt.plot(predictions * (7 / 500000), label='Predicted Prices', color='red')\n",
    "plt.legend()\n",
    "plt.title('Stock Price Prediction with PyTorch GRU')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
